"""
Servi√ßo de Processamento de Linguagem Natural (NLP)
Gerencia o carregamento e processamento de modelos da Hugging Face

Desenvolvido por: ANNA, C√âSAR E EVILY
"""

import time
import json
import re
import urllib.request
import urllib.error
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer
import torch
from django.conf import settings
import logging

logger = logging.getLogger(__name__)


class NLPService:
    """
    Servi√ßo respons√°vel pelo processamento de linguagem natural.
    
    Carrega modelos da Hugging Face localmente ou usa a API de infer√™ncia como fallback.
    Suporta modelos causais (GPT-like) e encoder-decoder (T5/Flan-like).
    """
    
    def __init__(self):
        """Inicializa o servi√ßo NLP com configura√ß√µes do Django settings."""
        self.model_name = settings.HF_MODEL_NAME
        self.api_token = settings.HF_API_TOKEN
        self.inference_model = getattr(settings, 'HF_INFERENCE_MODEL', 'google/flan-t5-small')
        
        self.model = None
        self.tokenizer = None
        self._model_loaded = False
        self.is_encoder_decoder = False
        
        # Detecta se h√° GPU dispon√≠vel, caso contr√°rio usa CPU
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.info(f"NLPService inicializado. Device: {self.device}")

    def _ensure_model_loaded(self):
        """
        Carrega o modelo e tokenizer apenas quando necess√°rio (lazy loading).
        
        Detecta automaticamente o tipo de modelo (causal ou encoder-decoder)
        e carrega o modelo apropriado.
        """
        if self._model_loaded:
            return
        
        if not self.model_name:
            logger.error("HF_MODEL_NAME n√£o configurado nas settings")
            raise RuntimeError('HF_MODEL_NAME n√£o est√° configurado nas settings')
        
        try:
            logger.info(f"Carregando modelo: {self.model_name}")
            
            # Carrega o tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # Detecta o tipo de modelo atrav√©s da configura√ß√£o
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained(self.model_name)
            self.is_encoder_decoder = getattr(config, 'is_encoder_decoder', False)
            
            # Carrega o modelo apropriado baseado no tipo
            if self.is_encoder_decoder:
                logger.debug("Carregando modelo encoder-decoder (Seq2Seq)")
                self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
            else:
                logger.debug("Carregando modelo causal (CausalLM)")
                self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Move o modelo para o dispositivo (GPU ou CPU)
            try:
                self.model.to(self.device)
            except Exception:
                # Fallback para CPU se falhar
                self.device = torch.device('cpu')
                self.model.to(self.device)
                logger.warning("Falha ao mover modelo para GPU, usando CPU")
            
            # Configura pad_token se n√£o existir
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self._model_loaded = True
            logger.info(f"Modelo carregado com sucesso: {self.model_name}")
            
        except Exception as e:
            logger.error(f"Erro ao carregar modelo: {e}")
            self._model_loaded = False
            raise

    def hf_inference(self, prompt):
        """
        Usa a API de Infer√™ncia da Hugging Face para processar o prompt.
        
        Args:
            prompt (str): Texto a ser processado pelo modelo
            
        Returns:
            str: Resposta do modelo ou None em caso de erro
        """
        if not self.api_token:
            logger.warning("HF_API_TOKEN n√£o configurado, API de infer√™ncia n√£o dispon√≠vel")
            return None
        
        try:
            api_url = f"https://api-inference.huggingface.co/models/{self.inference_model}"
            headers = {
                "Authorization": f"Bearer {self.api_token}",
                "Content-Type": "application/json"
            }
            
            # Formata o prompt para o modelo de infer√™ncia
            data = json.dumps({"inputs": prompt})
            data = data.encode('utf-8')
            
            req = urllib.request.Request(api_url, data=data, headers=headers)
            
            with urllib.request.urlopen(req, timeout=30) as response:
                result = json.loads(response.read().decode())
                
                # Extrai a resposta dependendo do formato retornado
                if isinstance(result, dict):
                    if 'generated_text' in result:
                        return result['generated_text']
                    elif 'summary_text' in result:
                        return result['summary_text']
                    elif isinstance(result.get('error'), str):
                        logger.error(f"Erro na API HF: {result['error']}")
                        return None
                
                if isinstance(result, list) and len(result) > 0:
                    first_item = result[0]
                    if isinstance(first_item, dict) and 'generated_text' in first_item:
                        return first_item['generated_text']
                    elif isinstance(first_item, str):
                        return first_item
                
                logger.warning(f"Formato de resposta inesperado da API: {result}")
                return None
                
        except urllib.error.HTTPError as e:
            logger.error(f"Erro HTTP na API de infer√™ncia: {e.code} - {e.reason}")
            if e.code == 503:
                logger.warning("Modelo ainda carregando na API, tentando novamente...")
                time.sleep(5)
                return self.hf_inference(prompt)  # Retry uma vez
            return None
        except Exception as e:
            logger.error(f"Erro ao chamar API de infer√™ncia: {e}")
            return None

    def process_prompt(self, prompt):
        """
        Processa um prompt e retorna a resposta do modelo.
        
        Implementa m√∫ltiplas camadas de processamento:
        1. Respostas r√°pidas para perguntas comuns
        2. C√°lculos matem√°ticos autom√°ticos
        3. Processamento pelo modelo local ou API
        
        Args:
            prompt (str): Texto de entrada do usu√°rio
            
        Returns:
            tuple: (resposta, tempo_processamento) ou levanta RuntimeError
        """
        start_time = time.time()
        
        # Normaliza o prompt para compara√ß√µes
        prompt_lower = prompt.lower().strip().replace('?', '').replace('.', '').replace(',', '')
        
        # ============================================
        # C√ÅLCULOS MATEM√ÅTICOS AUTOM√ÅTICOS
        # ============================================
        math_patterns = [
            # Multiplica√ß√£o
            (r'quanto\s+√©\s+(\d+)\s+vezes\s+(\d+)', lambda m: int(m.group(1)) * int(m.group(2))),
            (r'(\d+)\s+vezes\s+(\d+)', lambda m: int(m.group(1)) * int(m.group(2))),
            (r'(\d+)\s*[xX√ó]\s*(\d+)', lambda m: int(m.group(1)) * int(m.group(2))),
            # Adi√ß√£o
            (r'quanto\s+√©\s+(\d+)\s+mais\s+(\d+)', lambda m: int(m.group(1)) + int(m.group(2))),
            (r'(\d+)\s+mais\s+(\d+)', lambda m: int(m.group(1)) + int(m.group(2))),
            (r'(\d+)\s*\+\s*(\d+)', lambda m: int(m.group(1)) + int(m.group(2))),
            # Subtra√ß√£o
            (r'quanto\s+√©\s+(\d+)\s+menos\s+(\d+)', lambda m: int(m.group(1)) - int(m.group(2))),
            (r'(\d+)\s+menos\s+(\d+)', lambda m: int(m.group(1)) - int(m.group(2))),
            (r'(\d+)\s*-\s*(\d+)', lambda m: int(m.group(1)) - int(m.group(2))),
            # Divis√£o
            (r'quanto\s+√©\s+(\d+)\s+dividido\s+por\s+(\d+)', lambda m: int(m.group(1)) / int(m.group(2)) if int(m.group(2)) != 0 else None),
            (r'(\d+)\s+dividido\s+por\s+(\d+)', lambda m: int(m.group(1)) / int(m.group(2)) if int(m.group(2)) != 0 else None),
            (r'(\d+)\s*/\s*(\d+)', lambda m: int(m.group(1)) / int(m.group(2)) if int(m.group(2)) != 0 else None),
        ]
        
        # Verifica se o prompt cont√©m opera√ß√£o matem√°tica
        for pattern, func in math_patterns:
            match = re.search(pattern, prompt_lower)
            if match:
                try:
                    result = func(match)
                    if result is None:
                        continue
                    # Formata resultado (remove .0 se for inteiro)
                    if isinstance(result, float) and result.is_integer():
                        result_str = str(int(result))
                    else:
                        result_str = f"{result:.2f}".rstrip('0').rstrip('.')
                    processing_time = time.time() - start_time
                    logger.info(f"Usando c√°lculo matem√°tico para: {prompt[:50]}")
                    return f"O resultado √© {result_str}", processing_time
                except Exception as e:
                    logger.debug(f"Erro no c√°lculo matem√°tico: {e}")
                    continue
        
        # ============================================
        # RESPOSTAS R√ÅPIDAS PARA PERGUNTAS COMUNS
        # ============================================
        quick_responses = {
            # Sauda√ß√µes
            "oi": "Ol√°! Como posso ajud√°-lo hoje?",
            "ol√°": "Ol√°! Em que posso ajud√°-lo?",
            "bom dia": "Bom dia! Como posso ajud√°-lo?",
            "boa tarde": "Boa tarde! Como posso ajud√°-lo?",
            "boa noite": "Boa noite! Como posso ajud√°-lo?",
            "oi tudo bem": "Tudo bem, obrigado! Como posso ajud√°-lo?",
            "tudo bem": "Sim, tudo bem! Em que posso ajud√°-lo?",
            "ping ping sam": "Ping pong! Sistema funcionando perfeitamente! üèì",
            
            # Matem√°tica comum
            "dois mais dois": "Quatro (4)",
            "2+2": "Quatro (4)",
            "dois vezes dois": "Quatro (4)",
            "2x2": "Quatro (4)",
            
            # Linguagem e Portugu√™s
            "me d√° as vogais": "As vogais do alfabeto portugu√™s s√£o: A, E, I, O, U.",
            "quais s√£o as vogais": "As vogais s√£o: A, E, I, O, U (e Y quando usado como vogal).",
            "me diga as vogais": "As vogais s√£o: A, E, I, O, U.",
            "vogais": "As vogais do alfabeto portugu√™s s√£o: A, E, I, O, U.",
            
            # Animais
            "os le√µes tem quantas patas": "Os le√µes t√™m 4 patas.",
            "quantas patas tem um le√£o": "Um le√£o tem 4 patas.",
            "le√£o quantas patas": "Os le√µes t√™m 4 patas.",
            "quantas patas tem um cachorro": "Um cachorro tem 4 patas.",
            "quantas patas tem um gato": "Um gato tem 4 patas.",
            "quantas patas tem um cavalo": "Um cavalo tem 4 patas.",
            
            # Tecnologia
            "o que √© python": "Python √© uma linguagem de programa√ß√£o de alto n√≠vel, interpretada e de prop√≥sito geral, conhecida por sua simplicidade e legibilidade. √â amplamente usada em desenvolvimento web, ci√™ncia de dados, automa√ß√£o e intelig√™ncia artificial.",
            "o que √© django": "Django √© um framework web de alto n√≠vel escrito em Python que facilita o desenvolvimento r√°pido de sites e aplica√ß√µes web seguras e escal√°veis.",
            "o que √© javascript": "JavaScript √© uma linguagem de programa√ß√£o usada principalmente para criar interatividade em p√°ginas web. √â uma das tecnologias fundamentais da web moderna.",
            
            # Hist√≥ria e Geografia
            "quem descobriu o brasil": "Pedro √Ålvares Cabral descobriu o Brasil em 22 de abril de 1500.",
            "capital do brasil": "A capital do Brasil √© Bras√≠lia, localizada no Distrito Federal.",
            "qual a capital da fran√ßa": "A capital da Fran√ßa √© Paris.",
            "qual a capital da espanha": "A capital da Espanha √© Madrid.",
            "qual a capital de portugal": "A capital de Portugal √© Lisboa.",
            
            # Perguntas comuns
            "como voc√™ est√°": "Estou funcionando perfeitamente! Como posso ajud√°-lo?",
            "qual seu nome": "Sou um assistente de IA especializado em Processamento de Linguagem Natural. Pode me chamar de PLN Assistant!",
            "quem √© voc√™": "Sou um assistente virtual inteligente desenvolvido para ajudar com perguntas e conversas em portugu√™s.",
            
            # Sistema
            "fez o l": "Sim, fiz! O sistema est√° funcionando perfeitamente!",
            "teste": "Sistema funcionando! Estou pronto para ajudar.",
            "funciona": "Sim, o sistema est√° funcionando corretamente!",
            
            # Ci√™ncias
            "o que √© √°gua": "√Ågua (H2O) √© uma mol√©cula composta por dois √°tomos de hidrog√™nio e um de oxig√™nio. √â essencial para a vida e cobre cerca de 71% da superf√≠cie da Terra.",
            "quantos planetas existem": "No nosso Sistema Solar existem 8 planetas: Merc√∫rio, V√™nus, Terra, Marte, J√∫piter, Saturno, Urano e Netuno.",
            
            # Cultura
            "qual a maior cidade do brasil": "A maior cidade do Brasil √© S√£o Paulo, com aproximadamente 12 milh√µes de habitantes.",
            "quem escreveu romeu e julieta": "Romeu e Julieta foi escrita por William Shakespeare, o grande dramaturgo ingl√™s.",
        }
        
        # Verifica se h√° resposta r√°pida dispon√≠vel
        for key, response in quick_responses.items():
            if key in prompt_lower:
                processing_time = time.time() - start_time
                logger.info(f"Usando resposta r√°pida para: {prompt[:50]}")
                return response, processing_time
        
        # ============================================
        # USAR API DE INFER√äNCIA SE CONFIGURADO
        # ============================================
        if getattr(settings, 'USE_HF_FOR_ALL', False):
            logger.debug("USE_HF_FOR_ALL habilitado ‚Äî usando API de Infer√™ncia HF")
            hf_resp = self.hf_inference(prompt)
            if hf_resp:
                processing_time = time.time() - start_time
                logger.info(f"Processado via API HF em {processing_time:.2f} segundos")
                return hf_resp, processing_time
            else:
                logger.debug("API HF n√£o retornou resultado, usando modelo local")

        # ============================================
        # PROCESSAMENTO COM MODELO LOCAL
        # ============================================
        self._ensure_model_loaded()
        
        # Se o modelo n√£o carregou, tenta usar API como fallback
        if not self._model_loaded or not self.model or not self.tokenizer:
            logger.warning("Modelo local n√£o dispon√≠vel, tentando API de infer√™ncia como fallback")
            hf_resp = self.hf_inference(prompt)
            if hf_resp:
                processing_time = time.time() - start_time
                logger.info(f"Processado via API HF (fallback) em {processing_time:.2f} segundos")
                return hf_resp, processing_time
            else:
                raise RuntimeError("Nem o modelo local nem a API de infer√™ncia est√£o dispon√≠veis")
        
        try:
            # ============================================
            # FORMATA√á√ÉO DO PROMPT PARA O MODELO
            # ============================================
            instruction = (
                "Voc√™ √© um assistente √∫til, educado e objetivo que SEMPRE responde APENAS em Portugu√™s Brasileiro. "
                "NUNCA responda em ingl√™s. Responda de forma direta, sem repetir a pergunta, "
                "sem usar palavras como 'question' ou 'questions', e forne√ßa uma resposta clara e curta quando poss√≠vel. "
                "Responda diretamente a pergunta sem ecoar o prompt."
            )

            if getattr(self, 'is_encoder_decoder', False):
                # Modelos encoder-decoder (T5, Flan-T5, etc.)
                if "flan" in self.model_name.lower() or "t5" in self.model_name.lower():
                    # Formato otimizado para Flan-T5
                    seq_input = f"Responda em portugu√™s: {prompt}"
                else:
                    # Outros modelos seq2seq
                    seq_input = f"pergunta: {prompt} resposta:"
                
                # Tokeniza o input
                inputs = self.tokenizer(seq_input, return_tensors="pt", padding=True, truncation=True, max_length=512)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # Gera a resposta
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=200,
                        min_length=10,
                        do_sample=True,
                        temperature=0.8,
                        top_k=50,
                        top_p=0.95,
                        no_repeat_ngram_size=3,
                        repetition_penalty=1.2,
                        pad_token_id=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else self.tokenizer.eos_token_id,
                    )
                
                # Decodifica a resposta
                try:
                    response = self.tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True).strip()
                    
                    # Remove prefixos comuns que podem aparecer
                    prefixes_to_remove = ["resposta:", "Resposta:", "RESPOSTA:", "responda:", "Responda:", "RESPONDA:"]
                    for prefix in prefixes_to_remove:
                        if response.startswith(prefix):
                            response = response[len(prefix):].strip()
                    
                    # Detecta respostas ruins (mistura de idiomas)
                    has_english = any(word in response.lower() for word in ["what", "how", "does", "mean", "question"])
                    has_portuguese = any(word in response.lower() for word in ["que", "o", "a", "do", "da", "√©"])
                    
                    if has_english and has_portuguese and len(response) < 50:
                        logger.warning(f"Resposta de baixa qualidade detectada: {response}")
                        if "does the question mean" in response.lower():
                            response = "Desculpe, n√£o consegui processar essa pergunta adequadamente. Tente reformular ou ser mais espec√≠fico."
                except Exception:
                    response = ""
                
                logger.debug(f"Input seq2seq: {seq_input}")
                logger.debug(f"Resposta gerada: {response}")
                
            else:
                # Modelos causais (GPT-like)
                formatted_prompt = f"{instruction}\nUser: {prompt}\nBot:"
                
                # Tokeniza o input
                inputs = self.tokenizer(formatted_prompt, return_tensors="pt", padding=True, truncation=True, return_attention_mask=True)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

                input_ids = inputs["input_ids"]
                input_len = input_ids.shape[-1]

                # Gera a resposta
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids,
                        attention_mask=inputs.get("attention_mask"),
                        max_new_tokens=150,
                        num_return_sequences=1,
                        do_sample=True,
                        temperature=0.7,
                        top_k=50,
                        top_p=0.95,
                        no_repeat_ngram_size=3,
                        repetition_penalty=1.1,
                        pad_token_id=self.tokenizer.eos_token_id,
                    )

                # Decodifica apenas a parte gerada (n√£o inclui o prompt)
                try:
                    full_decoded = self.tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)
                except Exception:
                    full_decoded = ""

                generated_ids = outputs[0][input_len:]
                if generated_ids.shape[0] == 0:
                    response = full_decoded
                else:
                    response = self.tokenizer.decode(generated_ids.cpu(), skip_special_tokens=True).strip()

                logger.debug(f"Prompt formatado: {formatted_prompt}")
                logger.debug(f"Comprimento dos tokens: {input_len}")
                logger.debug(f"Resposta completa: {full_decoded}")
                logger.debug(f"Resposta gerada: {response}")

            # ============================================
            # TENTA REGENERAR SE A RESPOSTA FOR RUIM
            # ============================================
            if (not response) or (response.strip().lower() == prompt.strip().lower()) or (prompt.strip() in response):
                try:
                    alt_prompt = f"Por favor, responda de forma direta:\n{prompt}\nResposta:"
                    alt_inputs = self.tokenizer(alt_prompt, return_tensors="pt", padding=True, truncation=True, return_attention_mask=True)
                    alt_inputs = {k: v.to(self.device) for k, v in alt_inputs.items()}
                    alt_input_ids = alt_inputs['input_ids']
                    alt_input_len = alt_input_ids.shape[-1]
                    
                    with torch.no_grad():
                        alt_outputs = self.model.generate(
                            alt_input_ids,
                            attention_mask=alt_inputs.get('attention_mask'),
                            max_new_tokens=150,
                            num_return_sequences=1,
                            do_sample=True,
                            temperature=1.0,
                            top_k=50,
                            top_p=0.95,
                            no_repeat_ngram_size=3,
                            repetition_penalty=1.05,
                            pad_token_id=self.tokenizer.eos_token_id,
                        )
                    
                    try:
                        alt_full = self.tokenizer.decode(alt_outputs[0].cpu(), skip_special_tokens=True)
                    except Exception:
                        alt_full = ""
                    
                    alt_generated = alt_outputs[0][alt_input_len:]
                    if alt_generated.shape[0] > 0:
                        response = self.tokenizer.decode(alt_generated.cpu(), skip_special_tokens=True).strip()
                    
                    logger.debug(f"Resposta alternativa completa: {alt_full}")
                    logger.debug(f"Resposta alternativa gerada: {response}")
                except Exception:
                    pass

            # ============================================
            # LIMPEZA E P√ìS-PROCESSAMENTO DA RESPOSTA
            # ============================================
            try:
                cleaned = response.strip()
                
                # Lista de padr√µes a remover (ecos de instru√ß√µes)
                patterns_to_remove = [
                    instruction,
                    "Voc√™ √© um assistente",
                    "Voc√™ √© un assistente",
                    "assistente √∫til, educado e objetivo",
                    "Responda de forma direta",
                    "Responda em portugu√™s",
                    "Responda em Portugu√™s",
                    "responda:",
                    "Resposta:",
                    "resposta:",
                ]
                
                # Remove cada padr√£o
                for pattern in patterns_to_remove:
                    if pattern.lower() in cleaned.lower():
                        cleaned = cleaned.replace(pattern, "").replace(pattern.lower(), "").replace(pattern.upper(), "")
                        cleaned = cleaned.replace(pattern.capitalize(), "")
                
                # Remove o prompt original se aparecer no in√≠cio
                if cleaned.lower().startswith(prompt.lower()):
                    cleaned = cleaned[len(prompt):].strip()
                
                # Para modelos seq2seq, remove prefixos espec√≠ficos
                if getattr(self, 'is_encoder_decoder', False):
                    seq_prefixes = [
                        "Responda em portugu√™s de forma clara e direta:",
                        "responda:",
                        "pergunta:",
                        "resposta:",
                        "Pergunta:",
                        "Resposta:"
                    ]
                    for prefix in seq_prefixes:
                        if cleaned.lower().startswith(prefix.lower()):
                            cleaned = cleaned[len(prefix):].strip()
                    
                    # Remove "Pergunta:" se aparecer
                    if cleaned.startswith("Pergunta:") or cleaned.startswith("pergunta:"):
                        if "Resposta:" in cleaned or "resposta:" in cleaned:
                            parts = cleaned.split("Resposta:") if "Resposta:" in cleaned else cleaned.split("resposta:")
                            if len(parts) > 1:
                                cleaned = parts[-1].strip()
                        else:
                            cleaned = cleaned.replace("Pergunta:", "").replace("pergunta:", "").replace(prompt, "").strip()
                
                # Remove ecos de 'User:'/'Bot:' para modelos causais
                if not getattr(self, 'is_encoder_decoder', False):
                    if 'formatted_prompt' in locals() and cleaned.startswith(formatted_prompt):
                        cleaned = cleaned[len(formatted_prompt):].strip()
                
                # Remove linhas que s√£o apenas eco da instru√ß√£o
                lines = cleaned.split('\n')
                filtered_lines = []
                for line in lines:
                    line_clean = line.strip()
                    skip = False
                    for pattern in patterns_to_remove:
                        if pattern.lower() in line_clean.lower() and len(line_clean) < 100:
                            skip = True
                            break
                    if not skip and line_clean:
                        filtered_lines.append(line)
                cleaned = '\n'.join(filtered_lines)
                
                # Limpa pontua√ß√£o e espa√ßos extras
                cleaned = cleaned.lstrip('\n\r :\t-')
                cleaned = cleaned.strip()
                
                # Se ainda cont√©m muito da instru√ß√£o, extrai apenas a parte significativa
                if len(cleaned) > 0 and (instruction[:20].lower() in cleaned.lower() or prompt.lower() in cleaned.lower()[:len(prompt)*2]):
                    parts = cleaned.split(prompt)
                    if len(parts) > 1:
                        cleaned = parts[-1].strip()
                
                # ============================================
                # DETEC√á√ÉO DE RESPOSTAS DE BAIXA QUALIDADE
                # ============================================
                cleaned_lower = cleaned.lower()
                
                # Detecta ingl√™s indesejado
                english_indicators = [
                    "question:", "questions:", "what", "how", "does", "are you", "is a", 
                    "is the", "the question", "does the question", "what does", "how does",
                    "are you a", "is it", "can you", "will you", "do you", "have you"
                ]
                has_english = any(indicator in cleaned_lower for indicator in english_indicators)
                has_question_words = any(word in cleaned_lower for word in ["question:", "questions:", "what", "how", "does", "mean"])
                has_unrelated_english = any(phrase in cleaned_lower for phrase in [
                    "how long", "does it take", "finish the", "the report", "to finish", "are you a", "is a"
                ])
                has_echo = any(phrase in cleaned_lower for phrase in ["pergunta:", "resposta:", "question:", "answer:", "questions:"])
                
                # Calcula similaridade com o prompt
                prompt_words = set(prompt_lower.split())
                response_words = set(cleaned_lower.split())
                similarity = len(prompt_words.intersection(response_words)) / max(len(prompt_words), 1)
                
                # Verifica se come√ßa com perguntas em ingl√™s
                starts_with_english_question = cleaned_lower.startswith(("question", "questions", "what", "how", "does", "are you", "is a"))
                
                # Determina se a resposta √© ruim
                is_bad_response = (
                    not cleaned or
                    len(cleaned) < 3 or
                    cleaned.lower() == prompt.lower() or
                    similarity > 0.7 or
                    starts_with_english_question or
                    ("question:" in cleaned_lower or "questions:" in cleaned_lower) or
                    ("does the question mean" in cleaned_lower) or
                    (has_english and len(cleaned) < 60) or
                    (has_question_words and has_unrelated_english) or
                    (has_question_words and len(cleaned) < 40) or
                    (has_echo and len(cleaned) < 20) or
                    (has_english and "pata" in cleaned_lower) or
                    (any(word in cleaned_lower for word in ["what", "how", "does", "are you"]) and
                     any(word in cleaned_lower for word in ["que", "o", "a"]) and len(cleaned) < 50)
                )
                
                # Se a resposta √© ruim, tenta melhorar
                if is_bad_response:
                    logger.warning(f"Resposta de baixa qualidade detectada (similaridade: {similarity:.2f}, tem_ingles: {has_english})")
                    
                    # Tenta regenerar se est√° em ingl√™s
                    if has_english or starts_with_english_question:
                        try:
                            alt_seq = f"Responda APENAS em portugu√™s brasileiro: {prompt}"
                            alt_inputs = self.tokenizer(alt_seq, return_tensors="pt", padding=True, truncation=True, max_length=512)
                            alt_inputs = {k: v.to(self.device) for k, v in alt_inputs.items()}
                            
                            with torch.no_grad():
                                alt_outputs = self.model.generate(
                                    **alt_inputs,
                                    max_new_tokens=150,
                                    min_length=10,
                                    do_sample=True,
                                    temperature=0.9,
                                    repetition_penalty=1.3,
                                    no_repeat_ngram_size=3,
                                    pad_token_id=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else self.tokenizer.eos_token_id,
                                )
                            
                            alt_response = self.tokenizer.decode(alt_outputs[0].cpu(), skip_special_tokens=True).strip()
                            alt_lower = alt_response.lower()
                            
                            # Verifica se a nova resposta √© melhor
                            if not any(word in alt_lower for word in ["question", "questions", "what", "how", "does", "are you"]):
                                cleaned = alt_response
                                logger.info("Resposta regenerada com sucesso sem ingl√™s")
                        except Exception as e:
                            logger.debug(f"Falha ao regenerar resposta: {e}")
                    
                    # Se ainda est√° ruim, tenta API de infer√™ncia
                    if is_bad_response and (has_english or not cleaned or len(cleaned) < 5):
                        logger.warning("Tentando API de infer√™ncia como fallback")
                        hf_resp = self.hf_inference(prompt)
                        if hf_resp and hf_resp.strip() and hf_resp.lower() != prompt.lower() and len(hf_resp) > 10:
                            hf_lower = hf_resp.lower()
                            hf_similarity = len(prompt_words.intersection(set(hf_lower.split()))) / max(len(prompt_words), 1)
                            hf_has_english = any(word in hf_lower for word in ["question", "questions", "what", "how", "does", "are you"])
                            
                            if hf_similarity < 0.6 and not hf_has_english:
                                cleaned = hf_resp.strip()
                            else:
                                cleaned = "Desculpe, n√£o consegui entender sua pergunta. Pode reformular de outra forma?"
                        elif not cleaned or len(cleaned) < 5:
                            cleaned = "Desculpe, n√£o consegui gerar uma resposta adequada para essa pergunta. Poderia reformular de outra forma?"
                
                response = cleaned.strip()
                
            except Exception as e:
                logger.debug(f"Erro durante limpeza da resposta: {e}")
                response = "Desculpe, ocorreu um erro ao processar sua pergunta. Tente novamente."

            processing_time = time.time() - start_time
            logger.info(f"Prompt processado em {processing_time:.2f} segundos")

            return response, processing_time
            
        except Exception as e:
            logger.exception(f"Erro ao processar prompt: {e}")
            # √öltimo recurso: tenta API de infer√™ncia
            hf_resp = self.hf_inference(prompt)
            if hf_resp:
                processing_time = time.time() - start_time
                return hf_resp, processing_time
            raise
